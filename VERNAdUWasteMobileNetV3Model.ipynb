{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### VERN AdU Waste MobileNetV3 Small Model"
      ],
      "metadata": {
        "id": "kET0RyfJeYqG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekxEehgJHOkq"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install --upgrade pip -q\n",
        "!pip install tensorflow==2.16.1 tf-keras==2.16.0 scikit-learn==1.4.2 matplotlib seaborn pillow -q\n",
        "print(\"Libraries installation completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pG08IxNALLtZ"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV3Small\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras import backend as K\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image, ImageFile\n",
        "from google.colab import drive, files\n",
        "\n",
        "# Allow processing of truncated images\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "print(\"Libraries imported successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOqoBiLnMsYj"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to mount Google Drive: {e}\")\n",
        "\n",
        "# Set paths for dataset and merged dataset\n",
        "google_drive_path = '/content/drive/MyDrive/CSRP/dataSet'\n",
        "merged_dataset_dir = '/content/merged_dataset'\n",
        "os.makedirs(merged_dataset_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ul9JKoA6mp_"
      },
      "outputs": [],
      "source": [
        "# Function to copy and validate images, with enhanced error handling\n",
        "def copy_and_validate_images(source_dir, subcategories, category, target_dir):\n",
        "    try:\n",
        "        target_category_dir = os.path.join(target_dir, category)\n",
        "        os.makedirs(target_category_dir, exist_ok=True)\n",
        "        total_images_in_category = 0\n",
        "\n",
        "        for subcategory in subcategories:\n",
        "            subcategory_dir = os.path.join(source_dir, category, subcategory)\n",
        "            image_files = os.listdir(subcategory_dir)\n",
        "            total_images = len(image_files)\n",
        "            total_images_in_category += total_images\n",
        "            print(f\"Processing {total_images} images in '{subcategory}' under '{category}'.\")\n",
        "\n",
        "            for img_file in image_files:\n",
        "                source_file_path = os.path.join(subcategory_dir, img_file)\n",
        "                target_file_path = os.path.join(target_category_dir, img_file)\n",
        "                if os.path.exists(target_file_path):\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    with Image.open(source_file_path) as img:\n",
        "                        if img.mode in ('P', 'PA'):\n",
        "                            img = img.convert('RGBA')\n",
        "                        if img.mode == 'RGBA':\n",
        "                            img = img.convert('RGB')\n",
        "                        elif img.mode == 'LA':\n",
        "                            img = img.convert('L')\n",
        "                        if img.mode not in ('RGB', 'L'):\n",
        "                            img = img.convert('RGB')\n",
        "                        img.save(target_file_path)\n",
        "                except (IOError, OSError) as e:\n",
        "                    print(f\"Skipped image '{img_file}' due to: {e}\")\n",
        "\n",
        "            print(f\"Completed '{subcategory}' in '{category}'.\")\n",
        "        print(f\"Total images processed in '{category}': {total_images_in_category}\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing '{category}': {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3K9DZ7XMukb"
      },
      "outputs": [],
      "source": [
        "# Define subcategories for each waste type\n",
        "biodegradable_subcategories = ['food_waste', 'leaf_waste', 'paper_waste', 'wood_waste']\n",
        "non_biodegradable_subcategories = ['ewaste', 'metal_cans', 'other', 'plastic_bags', 'plastic_bottles']\n",
        "recyclable_subcategories = ['aluminum', 'carton', 'foam_box', 'milk_box', 'other', 'paper', 'paper_cup', 'plastic', 'plastic_cup']\n",
        "\n",
        "# Copy and validate images\n",
        "copy_and_validate_images(google_drive_path, biodegradable_subcategories, 'biodegradable', merged_dataset_dir)\n",
        "copy_and_validate_images(google_drive_path, non_biodegradable_subcategories, 'non_biodegradable', merged_dataset_dir)\n",
        "copy_and_validate_images(google_drive_path, recyclable_subcategories, 'recyclable', merged_dataset_dir)\n",
        "print(\"Dataset merging completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6AXEJgQoXqq"
      },
      "outputs": [],
      "source": [
        "# Data Augmentation and Preprocessing\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255, rotation_range=45, width_shift_range=0.2, height_shift_range=0.2,\n",
        "    shear_range=0.2, zoom_range=(0.8, 1.2), horizontal_flip=True, vertical_flip=True,\n",
        "    brightness_range=[0.7, 1.3], fill_mode='nearest', validation_split=0.2\n",
        ")\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Create train, validation, and test generators\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    merged_dataset_dir, target_size=(224, 224), batch_size=32, class_mode='categorical', subset='training'\n",
        ")\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    merged_dataset_dir, target_size=(224, 224), batch_size=32, class_mode='categorical', subset='validation'\n",
        ")\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    merged_dataset_dir, target_size=(224, 224), batch_size=32, class_mode='categorical'\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {train_generator.samples}, Validation samples: {validation_generator.samples}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbrnwVooMwNQ"
      },
      "outputs": [],
      "source": [
        "# Compute class weights to handle class imbalance\n",
        "try:\n",
        "    class_weights = compute_class_weight(\n",
        "        class_weight='balanced', classes=np.unique(train_generator.classes), y=train_generator.classes\n",
        "    )\n",
        "    class_weights = dict(enumerate(class_weights))\n",
        "    print(\"Class weights calculated.\")\n",
        "except Exception as e:\n",
        "    print(f\"Class weight calculation error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FerqVOXlM5Kh"
      },
      "outputs": [],
      "source": [
        "# Model building with improved architecture\n",
        "try:\n",
        "    base_model = MobileNetV3Small(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
        "    num_layers_to_freeze = int(0.05 * len(base_model.layers))\n",
        "    for layer in base_model.layers[:num_layers_to_freeze]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    x = GlobalAveragePooling2D()(base_model.output)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dense(512, activation='relu', kernel_regularizer=l2(1e-4))(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "    predictions = Dense(train_generator.num_classes, activation='softmax')(x)\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "    initial_lr = 1e-4\n",
        "    lr_schedule = ExponentialDecay(initial_learning_rate=initial_lr, decay_steps=5 * len(train_generator), decay_rate=0.8, staircase=True)\n",
        "    model.compile(optimizer=Adam(learning_rate=lr_schedule), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    print(\"Model built and compiled.\")\n",
        "except Exception as e:\n",
        "    print(f\"Model building or compilation error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TCCHHWsiY5C"
      },
      "outputs": [],
      "source": [
        "# Model training with enhanced validation and callbacks\n",
        "try:\n",
        "    history = model.fit(\n",
        "        train_generator, epochs=15, validation_data=validation_generator, class_weight=class_weights,\n",
        "        callbacks=[\n",
        "            ModelCheckpoint('best_model.keras', monitor='val_accuracy', save_best_only=True, mode='max'),\n",
        "            EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "        ]\n",
        "    )\n",
        "    print(\"Model training completed.\")\n",
        "except Exception as e:\n",
        "    print(f\"Model training error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HckW_QpOERn"
      },
      "outputs": [],
      "source": [
        "# Model evaluation and performance metrics\n",
        "try:\n",
        "    test_loss, test_accuracy = model.evaluate(test_generator)\n",
        "    print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "    y_true = test_generator.classes\n",
        "    y_pred_classes = np.argmax(model.predict(test_generator), axis=1)\n",
        "    precision = precision_score(y_true, y_pred_classes, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred_classes, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred_classes, average='weighted', zero_division=0)\n",
        "    print(f\"Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")\n",
        "except Exception as e:\n",
        "    print(f\"Model evaluation error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGTh5Qu2YsXo"
      },
      "outputs": [],
      "source": [
        "# Plot training history for accuracy and loss\n",
        "try:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend(loc='upper left')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Model Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print(\"Training history plotted.\")\n",
        "except Exception as e:\n",
        "    print(f\"Plotting error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlfSDbrWXYgV"
      },
      "outputs": [],
      "source": [
        "# Generate and display confusion matrix\n",
        "try:\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=test_generator.class_indices, yticklabels=test_generator.class_indices)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "    print(\"Confusion matrix displayed.\")\n",
        "except Exception as e:\n",
        "    print(f\"Confusion matrix error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "rJMWOCRAKKqe"
      },
      "outputs": [],
      "source": [
        "# Clear TensorFlow session and convert to TensorFlow Lite model for deployment\n",
        "K.clear_session()\n",
        "\n",
        "try:\n",
        "    model.export('saved_model')\n",
        "    converter = tf.lite.TFLiteConverter.from_saved_model('saved_model')\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    converter.target_spec.supported_types = [tf.float16]\n",
        "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
        "    tflite_model = converter.convert()\n",
        "\n",
        "    # Save and download the TensorFlow Lite model\n",
        "    with open('waste_classifier.tflite', 'wb') as f:\n",
        "        f.write(tflite_model)\n",
        "    print(\"Model successfully converted to TensorFlow Lite.\")\n",
        "    files.download('waste_classifier.tflite')\n",
        "    print(\"TensorFlow Lite model downloaded.\")\n",
        "except Exception as e:\n",
        "    print(f\"TensorFlow Lite conversion error: {e}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
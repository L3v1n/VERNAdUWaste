{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### VERN AdU Waste MobileNetV3 Small Model"
      ],
      "metadata": {
        "id": "kET0RyfJeYqG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekxEehgJHOkq",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "!pip install --upgrade pip -q\n",
        "!pip install tensorflow==2.16.1 tf-keras==2.16.0 scikit-learn==1.4.2 matplotlib seaborn pillow -q\n",
        "print(\"Libraries installation completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pG08IxNALLtZ",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV3Small\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras import backend as K\n",
        "import numpy as np\n",
        "import os\n",
        "from collections import Counter\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image, ImageFile\n",
        "from google.colab import drive, files\n",
        "\n",
        "# Allow processing of truncated images\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "print(\"Libraries imported successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOqoBiLnMsYj",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive to access the dataset\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error mounting Google Drive: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ul9JKoA6mp_"
      },
      "outputs": [],
      "source": [
        "# Paths for dataset and output directories\n",
        "google_drive_path = '/content/drive/MyDrive/CSRP/dataSet'\n",
        "merged_dataset_dir = '/content/merged_dataset'\n",
        "os.makedirs(merged_dataset_dir, exist_ok=True)\n",
        "\n",
        "# Function to copy and validate images, with enhanced error handling\n",
        "def copy_and_validate_images(source_dir, subcategories, category, target_dir):\n",
        "    try:\n",
        "        target_category_dir = os.path.join(target_dir, category)\n",
        "        os.makedirs(target_category_dir, exist_ok=True)\n",
        "        total_images_in_category = 0\n",
        "\n",
        "        for subcategory in subcategories:\n",
        "            subcategory_dir = os.path.join(source_dir, category, subcategory)\n",
        "            image_files = os.listdir(subcategory_dir)\n",
        "            total_images = len(image_files)\n",
        "            total_images_in_category += total_images\n",
        "            print(f\"Processing {total_images} images in '{subcategory}' under '{category}'.\")\n",
        "\n",
        "            for img_file in image_files:\n",
        "                source_file_path = os.path.join(subcategory_dir, img_file)\n",
        "                target_file_path = os.path.join(target_category_dir, img_file)\n",
        "                if os.path.exists(target_file_path):\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    with Image.open(source_file_path) as img:\n",
        "                        if img.mode in ('P', 'PA'):\n",
        "                            img = img.convert('RGBA')\n",
        "                        if img.mode == 'RGBA':\n",
        "                            img = img.convert('RGB')\n",
        "                        elif img.mode == 'LA':\n",
        "                            img = img.convert('L')\n",
        "                        if img.mode not in ('RGB', 'L'):\n",
        "                            img = img.convert('RGB')\n",
        "                        img.save(target_file_path)\n",
        "                except (IOError, OSError) as e:\n",
        "                    print(f\"Skipped image '{img_file}' due to: {e}\")\n",
        "\n",
        "            print(f\"Completed '{subcategory}' in '{category}'.\")\n",
        "        print(f\"Total images processed in '{category}': {total_images_in_category}\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing '{category}': {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3K9DZ7XMukb",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Define subcategories for each waste category\n",
        "biodegradable_subcategories = ['food_waste', 'leaf_waste', 'paper_waste', 'wood_waste']\n",
        "non_biodegradable_subcategories = ['ewaste', 'metal_cans', 'other', 'plastic_bags', 'plastic_bottles']\n",
        "recyclable_subcategories = ['aluminum', 'carton', 'foam_box', 'milk_box', 'other', 'paper', 'paper_cup', 'plastic', 'plastic_cup']\n",
        "\n",
        "# Merge and validate the dataset\n",
        "copy_and_validate_images(google_drive_path, biodegradable_subcategories, 'biodegradable', merged_dataset_dir)\n",
        "copy_and_validate_images(google_drive_path, non_biodegradable_subcategories, 'non_biodegradable', merged_dataset_dir)\n",
        "copy_and_validate_images(google_drive_path, recyclable_subcategories, 'recyclable', merged_dataset_dir)\n",
        "print(\"Dataset merging completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6AXEJgQoXqq",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Data generators for training, validation, and testing\n",
        "try:\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        rotation_range=30,\n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1,\n",
        "        shear_range=0.1,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        brightness_range=[0.8, 1.2],\n",
        "        fill_mode='nearest',\n",
        "        validation_split=0.2\n",
        "    )\n",
        "\n",
        "    # Validation and test data generators (rescaling only)\n",
        "    validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    # Create training data generator\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        merged_dataset_dir,\n",
        "        target_size=(224, 224),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical',\n",
        "        subset='training',\n",
        "        seed=42\n",
        "    )\n",
        "    print(f\"Training data generator created with {train_generator.samples} samples.\")\n",
        "\n",
        "    # Create validation data generator\n",
        "    validation_generator = train_datagen.flow_from_directory(\n",
        "        merged_dataset_dir,\n",
        "        target_size=(224, 224),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical',\n",
        "        subset='validation',\n",
        "        seed=42\n",
        "    )\n",
        "    print(f\"Validation data generator created with {validation_generator.samples} samples.\")\n",
        "\n",
        "    # Create test data generator\n",
        "    test_generator = test_datagen.flow_from_directory(\n",
        "        merged_dataset_dir,\n",
        "        target_size=(224, 224),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical'\n",
        "    )\n",
        "    print(f\"Test data generator created with {test_generator.samples} samples.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error in creating data generators: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbrnwVooMwNQ",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Compute class weights for handling class imbalance\n",
        "try:\n",
        "    class_counts = Counter(train_generator.classes)\n",
        "    total_samples = len(train_generator.classes)\n",
        "    print(f\"Class distribution in training data: {class_counts}\")\n",
        "    print(f\"Total training samples: {total_samples}\")\n",
        "\n",
        "    # Compute and normalize class weights\n",
        "    class_weights = {\n",
        "        class_id: np.log(total_samples / count)\n",
        "        for class_id, count in class_counts.items()\n",
        "    }\n",
        "    max_weight = max(class_weights.values())\n",
        "    class_weights = {class_id: weight / max_weight for class_id, weight in class_weights.items()}\n",
        "    print(f\"Normalized class weights: {class_weights}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Class weight calculation error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build and compile the model with staged fine-tuning and exponential decay learning rate\n",
        "try:\n",
        "    base_model = MobileNetV3Small(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
        "\n",
        "    # Freeze 60% of layers for initial training (Stage 1)\n",
        "    for layer in base_model.layers[:int(0.6 * len(base_model.layers))]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # Add custom layers\n",
        "    x = GlobalAveragePooling2D()(base_model.output)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dense(1024, activation='relu', kernel_regularizer=l2(1e-5))(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "    predictions = Dense(train_generator.num_classes, activation='softmax')(x)\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "    # Exponential decay learning rate schedule\n",
        "    lr_schedule_stage_1 = ExponentialDecay(\n",
        "        initial_learning_rate=1e-4, decay_steps=3 * len(train_generator), decay_rate=0.9, staircase=True\n",
        "    )\n",
        "    model.compile(optimizer=Adam(learning_rate=lr_schedule_stage_1), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    print(\"Model compiled for Stage 1.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Model building error: {e}\")"
      ],
      "metadata": {
        "id": "w7gM6i35ukOX",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FerqVOXlM5Kh",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Train the model in Stage 1\n",
        "try:\n",
        "    history_stage_1 = model.fit(\n",
        "        train_generator,\n",
        "        epochs=10,\n",
        "        validation_data=validation_generator,\n",
        "        class_weight=class_weights,\n",
        "        callbacks=[\n",
        "            ModelCheckpoint('best_model_stage_1.keras', monitor='val_accuracy', save_best_only=True, mode='max'),\n",
        "            EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "        ]\n",
        "    )\n",
        "    print(\"Stage 1 training completed.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Stage 1 training error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TCCHHWsiY5C"
      },
      "outputs": [],
      "source": [
        "# Fine-tune more layers (Stage 2)\n",
        "try:\n",
        "    for layer in base_model.layers[:int(0.3 * len(base_model.layers))]:\n",
        "        layer.trainable = False\n",
        "    for layer in base_model.layers[int(0.3 * len(base_model.layers)):]:\n",
        "        layer.trainable = True\n",
        "\n",
        "    lr_schedule_stage_2 = ExponentialDecay(\n",
        "        initial_learning_rate=5e-5, decay_steps=3 * len(train_generator), decay_rate=0.9, staircase=True\n",
        "    )\n",
        "    model.compile(optimizer=Adam(learning_rate=lr_schedule_stage_2), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    print(\"Model recompiled for Stage 2.\")\n",
        "\n",
        "    history_stage_2 = model.fit(\n",
        "        train_generator,\n",
        "        epochs=10,\n",
        "        validation_data=validation_generator,\n",
        "        class_weight=class_weights,\n",
        "        callbacks=[\n",
        "            ModelCheckpoint('best_model_stage_2.keras', monitor='val_accuracy', save_best_only=True, mode='max'),\n",
        "            EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "        ]\n",
        "    )\n",
        "    print(\"Stage 2 fine-tuning completed.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Stage 2 fine-tuning error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tune all layers (Stage 3)\n",
        "try:\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = True\n",
        "\n",
        "    lr_schedule_stage_3 = ExponentialDecay(\n",
        "        initial_learning_rate=1e-5, decay_steps=3 * len(train_generator), decay_rate=0.9, staircase=True\n",
        "    )\n",
        "    model.compile(optimizer=Adam(learning_rate=lr_schedule_stage_3), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    print(\"Model recompiled for Stage 3.\")\n",
        "\n",
        "    history_stage_3 = model.fit(\n",
        "        train_generator,\n",
        "        epochs=10,\n",
        "        validation_data=validation_generator,\n",
        "        class_weight=class_weights,\n",
        "        callbacks=[\n",
        "            ModelCheckpoint('best_model_stage_3.keras', monitor='val_accuracy', save_best_only=True, mode='max'),\n",
        "            EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "        ]\n",
        "    )\n",
        "    print(\"Stage 3 fine-tuning completed.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Stage 3 fine-tuning error: {e}\")"
      ],
      "metadata": {
        "id": "p1YEPbhYsUqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HckW_QpOERn"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model and compute metrics\n",
        "try:\n",
        "    test_loss, test_accuracy = model.evaluate(test_generator)\n",
        "    print(f\"Test Loss: {test_loss}\")\n",
        "    print(f\"Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "    # Generate predictions and compute metrics\n",
        "    y_true = test_generator.classes\n",
        "    y_pred_probs = model.predict(test_generator)\n",
        "    y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score\n",
        "    precision = precision_score(y_true, y_pred_classes, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred_classes, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred_classes, average='weighted', zero_division=0)\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "    print(f\"F1 Score: {f1}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Model evaluation error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGTh5Qu2YsXo"
      },
      "outputs": [],
      "source": [
        "# Plot training history for all stages\n",
        "try:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Plot accuracy\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history_stage_1.history['accuracy'], label='Stage 1 Train Acc')\n",
        "    plt.plot(history_stage_1.history['val_accuracy'], label='Stage 1 Val Acc')\n",
        "    plt.plot(history_stage_2.history['accuracy'], label='Stage 2 Train Acc')\n",
        "    plt.plot(history_stage_2.history['val_accuracy'], label='Stage 2 Val Acc')\n",
        "    plt.plot(history_stage_3.history['accuracy'], label='Stage 3 Train Acc')\n",
        "    plt.plot(history_stage_3.history['val_accuracy'], label='Stage 3 Val Acc')\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend(loc='upper left')\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history_stage_1.history['loss'], label='Stage 1 Train Loss')\n",
        "    plt.plot(history_stage_1.history['val_loss'], label='Stage 1 Val Loss')\n",
        "    plt.plot(history_stage_2.history['loss'], label='Stage 2 Train Loss')\n",
        "    plt.plot(history_stage_2.history['val_loss'], label='Stage 2 Val Loss')\n",
        "    plt.plot(history_stage_3.history['loss'], label='Stage 3 Train Loss')\n",
        "    plt.plot(history_stage_3.history['val_loss'], label='Stage 3 Val Loss')\n",
        "    plt.title('Model Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print(\"Training history plotted.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error plotting training history: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlfSDbrWXYgV"
      },
      "outputs": [],
      "source": [
        "# Generate and display the confusion matrix\n",
        "try:\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=test_generator.class_indices, yticklabels=test_generator.class_indices)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "    print(\"Confusion matrix displayed.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Confusion matrix error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJMWOCRAKKqe"
      },
      "outputs": [],
      "source": [
        "# Convert the model to TensorFlow Lite for deployment\n",
        "try:\n",
        "    K.clear_session()  # Clear the session to free up resources\n",
        "\n",
        "    # Export the trained model to a SavedModel format\n",
        "    model_dir = 'saved_model'\n",
        "    model.export(model_dir)\n",
        "    print(f\"Model exported successfully to directory: {model_dir}\")\n",
        "\n",
        "    # Convert the exported model to TensorFlow Lite\n",
        "    converter = tf.lite.TFLiteConverter.from_saved_model(model_dir)\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    converter.target_spec.supported_types = [tf.float16]\n",
        "    tflite_model = converter.convert()\n",
        "\n",
        "    # Save the TensorFlow Lite model\n",
        "    tflite_model_path = 'waste_classifier.tflite'\n",
        "    with open(tflite_model_path, 'wb') as f:\n",
        "        f.write(tflite_model)\n",
        "    print(f\"Model converted to TensorFlow Lite and saved as: {tflite_model_path}\")\n",
        "\n",
        "    # Download the TensorFlow Lite model file\n",
        "    files.download(tflite_model_path)\n",
        "    print(\"TensorFlow Lite model downloaded.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"TensorFlow Lite conversion error: {e}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}